{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Fine-tuning LLMs with LoRA\n",
        "\n",
        "This notebook demonstrates how to fine-tune a large language model (LLM) using Low-Rank Adaptation (LoRA) with the PEFT library. We'll use Mistral-7B as our base model and fine-tune it on a custom dataset.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment. We'll be using Google Colab's GPU runtime for this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q torch transformers datasets peft bitsandbytes accelerate tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Clone the Repository\n",
        "\n",
        "First, let's clone our repository to get access to our training scripts and configurations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/your-username/llm-finetuning-lora.git\n",
        "%cd llm-finetuning-lora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Import Dependencies\n",
        "\n",
        "Now let's import our custom modules and other required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from model.load_base_model import ModelLoader\n",
        "from data.prepare_dataset import DatasetPreparator\n",
        "from train.run_lora_finetune import run_training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "For this example, we'll use a small dummy dataset. In practice, you would replace this with your own custom dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize data preparator with Mistral tokenizer\n",
        "data_preparator = DatasetPreparator(\n",
        "    tokenizer=\"mistralai/Mistral-7B-v0.1\",\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "# Prepare dummy dataset\n",
        "dataset = data_preparator.prepare_dataset(use_dummy=True)\n",
        "print(f\"Prepared dataset with {len(dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Preparation\n",
        "\n",
        "Now let's load our base model and configure it with LoRA adapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and add LoRA adapter\n",
        "model_loader = ModelLoader(\"configs/lora_config.json\")\n",
        "model, tokenizer = model_loader.load_base_model()\n",
        "model = model_loader.add_lora_adapter(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we can start the training process. We'll use the training configuration from our config file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "run_training(\n",
        "    training_config_path=\"configs/training_args.json\",\n",
        "    lora_config_path=\"configs/lora_config.json\",\n",
        "    use_dummy_data=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Saving the Model\n",
        "\n",
        "The training script automatically saves the model checkpoints and final model to the specified output directory. You can find the saved model in the `outputs/checkpoints` directory.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now that we have fine-tuned our model, you can:\n",
        "\n",
        "1. Use the model for inference (see `2_test_finetuned.ipynb`)\n",
        "2. Fine-tune on your own custom dataset by replacing the dummy data\n",
        "3. Experiment with different LoRA configurations\n",
        "4. Try different base models\n",
        "\n",
        "Remember to clean up your runtime if you're using Google Colab to free up resources."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

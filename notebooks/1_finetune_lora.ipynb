{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Showmick119/Fine-Tuning-Open-Source-LLM/blob/main/notebooks/1_finetuning_lora.ipynb)\n",
        "\n",
        "# üöÄ Fine-tuning CodeLlama with QLoRA on CodeAlpaca\n",
        "\n",
        "This notebook demonstrates how to fine-tune **CodeLlama-7b-Instruct** using **QLoRA (4-bit quantization)** with the PEFT library on the **CodeAlpaca-20k** dataset. This approach allows efficient fine-tuning on a single GPU while maintaining high performance.\n",
        "\n",
        "## üéØ What You'll Learn\n",
        "\n",
        "- How to load and configure CodeLlama with 4-bit quantization\n",
        "- How to set up LoRA adapters for efficient fine-tuning\n",
        "- How to prepare the CodeAlpaca dataset for training\n",
        "- How to train with QLoRA and save checkpoints\n",
        "\n",
        "## üîß Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment. We'll be using Google Colab's GPU runtime for this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q torch transformers datasets peft bitsandbytes accelerate tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### üìÅ Clone the Repository\n",
        "\n",
        "First, let's clone our repository to get access to our training scripts, configurations, and the CodeAlpaca dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your actual repo URL)\n",
        "!git clone https://github.com/your-username/llm-finetuning-lora.git\n",
        "%cd llm-finetuning-lora\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"üöÄ CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### üì¶ Import Dependencies\n",
        "\n",
        "Now let's import our custom modules and other required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from model.load_base_model import ModelLoader\n",
        "from data.prepare_dataset import DatasetPreparator\n",
        "from train.run_lora_finetune import run_training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Data Preparation\n",
        "\n",
        "We'll use the **CodeAlpaca-20k** dataset, which contains 20,000 high-quality instruction-following examples specifically designed for code generation tasks. This dataset is perfect for fine-tuning CodeLlama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize data preparator with CodeLlama tokenizer\n",
        "print(\"üîß Initializing data preparator with CodeLlama tokenizer...\")\n",
        "data_preparator = DatasetPreparator(\n",
        "    tokenizer=\"codellama/CodeLlama-7b-Instruct-hf\",\n",
        "    max_length=512,\n",
        "    data_path=\"data/code_alpaca_20k.json\"\n",
        ")\n",
        "\n",
        "# Load and examine the CodeAlpaca dataset\n",
        "print(\"üìÅ Loading CodeAlpaca-20k dataset...\")\n",
        "dataset = data_preparator.prepare_dataset(use_dummy=False)\n",
        "print(f\"‚úÖ Prepared dataset with {len(dataset)} examples\")\n",
        "\n",
        "# Let's look at a sample from the dataset\n",
        "import json\n",
        "with open(\"data/code_alpaca_20k.json\", \"r\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"\\nüìù Sample from raw dataset:\")\n",
        "print(f\"Instruction: {raw_data[0]['instruction']}\")\n",
        "print(f\"Input: {raw_data[0]['input']}\")\n",
        "print(f\"Output: {raw_data[0]['output'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ñ Model Preparation with QLoRA\n",
        "\n",
        "Now let's load **CodeLlama-7b-Instruct** with **4-bit quantization** (QLoRA) and configure it with LoRA adapters. This dramatically reduces memory usage while maintaining training effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Load CodeLlama with 4-bit quantization and LoRA adapter\n",
        "print(\"üöÄ Loading CodeLlama-7b-Instruct with QLoRA configuration...\")\n",
        "model_loader = ModelLoader(\"configs/lora_config.json\")\n",
        "\n",
        "print(\"üì¶ Loading base model with 4-bit quantization...\")\n",
        "model, tokenizer = model_loader.load_base_model()\n",
        "\n",
        "print(\"üîó Adding LoRA adapter for efficient fine-tuning...\")\n",
        "model = model_loader.add_lora_adapter(model)\n",
        "\n",
        "# Check model memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üíæ GPU Memory Used: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"üíæ GPU Memory Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üèãÔ∏è Training with QLoRA\n",
        "\n",
        "Now we can start the fine-tuning process! We'll use the training configuration optimized for CodeLlama and QLoRA. The training will save checkpoints regularly so you can resume if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Start fine-tuning with QLoRA on CodeAlpaca dataset\n",
        "print(\"üèãÔ∏è Starting fine-tuning process...\")\n",
        "print(\"üìä Training on CodeAlpaca-20k dataset\")\n",
        "print(\"‚ö° Using QLoRA (4-bit quantization) for efficient training\")\n",
        "\n",
        "run_training(\n",
        "    training_config_path=\"configs/training_args.json\",\n",
        "    lora_config_path=\"configs/lora_config.json\",\n",
        "    data_path=\"data/code_alpaca_20k.json\",\n",
        "    use_dummy_data=False\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training completed successfully!\")\n",
        "print(\"üíæ Model checkpoints saved to: outputs/checkpoints\")\n",
        "print(\"üìã Training logs saved to: outputs/logs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üíæ Model Checkpoints & Next Steps\n",
        "\n",
        "The training script automatically saves:\n",
        "- **LoRA adapter weights** in `outputs/checkpoints/`\n",
        "- **Training logs** in `outputs/logs/`\n",
        "- **Tokenizer configuration** alongside the model\n",
        "\n",
        "## üéØ What's Next?\n",
        "\n",
        "Now that you have fine-tuned CodeLlama on CodeAlpaca, you can:\n",
        "\n",
        "1. **Test your model**: Use `2_test_model.ipynb` to interactively test code generation\n",
        "2. **Evaluate performance**: Use `3_evaluate_model.ipynb` to benchmark on HumanEval\n",
        "3. **Deploy your model**: Use the inference scripts for production deployment\n",
        "4. **Experiment further**: Try different LoRA configurations or datasets\n",
        "\n",
        "## üßπ Cleanup (Optional)\n",
        "\n",
        "If you're using Google Colab, you may want to free up GPU memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Clear GPU memory\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "del model\n",
        "del tokenizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"üßπ GPU memory cleared!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Showmick119/Fine-Tuning-Open-Source-LLM/blob/main/notebooks/test_finetuned.ipynb)\n",
        "\n",
        "# Testing the Fine-tuned Model\n",
        "\n",
        "This notebook demonstrates how to use your fine-tuned model for text generation. We'll load the model with the trained LoRA adapter and test it with various prompts.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q torch transformers peft bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Import Dependencies\n",
        "\n",
        "Let's import our text generation module and other required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from inference.generate_text import TextGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Initialize Text Generator\n",
        "\n",
        "Now let's initialize our text generator with the fine-tuned model. Make sure to update the `adapter_path` to point to your trained LoRA adapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the text generator\n",
        "generator = TextGenerator(\n",
        "    base_model_name=\"mistralai/Mistral-7B-v0.1\",\n",
        "    adapter_path=\"outputs/checkpoints\",  # Update this path\n",
        "    device=\"auto\",\n",
        "    load_8bit=True,  # Set to False if running on CPU\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Single Generation\n",
        "\n",
        "Let's try generating text with a single instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with a single instruction\n",
        "instruction = \"Write a poem about artificial intelligence\"\n",
        "response = generator.generate(instruction=instruction)\n",
        "\n",
        "print(\"Generated Response:\")\n",
        "print(\"-\" * 40)\n",
        "print(response)\n",
        "print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Batch Generation\n",
        "\n",
        "We can also generate responses for multiple instructions at once using batch generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text for multiple instructions\n",
        "instructions = [\n",
        "    \"Explain quantum computing in simple terms\",\n",
        "    \"Write a short story about space exploration\",\n",
        "    \"Describe the concept of machine learning\"\n",
        "]\n",
        "\n",
        "responses = generator.generate_batch(instructions=instructions)\n",
        "\n",
        "for i, (instruction, response) in enumerate(zip(instructions, responses), 1):\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Instruction: {instruction}\")\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response)\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test with Input Context\n",
        "\n",
        "Let's try generating text with both instruction and input context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text with instruction and input\n",
        "instruction = \"Summarize the following text\"\n",
        "input_text = \"\"\"\n",
        "Artificial intelligence has transformed various industries, from healthcare to transportation.\n",
        "Machine learning algorithms can now diagnose diseases, drive cars, and even create art.\n",
        "However, these advancements also raise important ethical questions about privacy, bias,\n",
        "and the future of human work.\n",
        "\"\"\"\n",
        "\n",
        "response = generator.generate(\n",
        "    instruction=instruction,\n",
        "    input_text=input_text,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "print(\"Generated Response:\")\n",
        "print(\"-\" * 40)\n",
        "print(response)\n",
        "print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Experiment with Generation Parameters\n",
        "\n",
        "Finally, let's try different generation parameters to see how they affect the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with different generation parameters\n",
        "instruction = \"Write a creative story about a time traveler\"\n",
        "\n",
        "# More deterministic output\n",
        "response_deterministic = generator.generate(\n",
        "    instruction=instruction,\n",
        "    temperature=0.3,\n",
        "    top_p=0.85,\n",
        "    top_k=40,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "# More creative output\n",
        "response_creative = generator.generate(\n",
        "    instruction=instruction,\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    top_k=60,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "\n",
        "print(\"Deterministic Response (temperature=0.3):\")\n",
        "print(\"-\" * 40)\n",
        "print(response_deterministic)\n",
        "print(\"\\nCreative Response (temperature=0.9):\")\n",
        "print(\"-\" * 40)\n",
        "print(response_creative)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

torch>=2.0.0
transformers>=4.36.0
datasets>=2.15.0
peft>=0.7.0
bitsandbytes>=0.41.0
accelerate>=0.25.0
tqdm>=4.66.0
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
wandb>=0.15.0  # Optional: for experiment tracking
jupyter>=1.0.0
lighteval>=0.4.0  # For HumanEval benchmarking (keeping for comparison)
evaluate>=0.4.0  # For evaluation metrics
requests>=2.31.0  # For GitHub API calls in dataset creator
openai>=1.0.0  # For LLM-as-judge evaluation
anthropic>=0.7.0  # Alternative LLM judge (optional)
fastapi>=0.104.0  # For testing generated code functionality
uvicorn>=0.24.0  # For FastAPI server testing
httpx>=0.25.0  # For API testing
